import pandas as pd
import numpy as np
from obspy import read
import os
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import pdb
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from sklearn.model_selection import train_test_split

# Load trace data given a SAC file
#def load_data(filename, see_stats=False, bandpass=True):
def load_data(filename, see_stats=False, bandpass=False):
    st = read(filename)
    tr = st[0]
    if bandpass:
        tr.filter(type='bandpass', freqmin=5.0, freqmax=40.0)
    # tr.normalize()
    tr.taper(0.2)
    if see_stats:
        print(tr.stats)
        tr.plot()
    return tr

# Find all event directories     
def find_all_events(path):
    dirs = []
    for r, d, f in os.walk(path):
        for name in d:
            dirs.append(os.path.join(r, name))
    return dirs

def find_all_SAC(path):
    files = []
    for r, d, f in os.walk(path):
        for file in f:
            if '.SAC' in file:
                files.append(os.path.join(r, file))
    return files

def get_event(path, station_no, showPlots=False):
#def get_event(path, station_no, showPlots=True):
	#sample_size = 8000
	# sample_size = 2000
	sample_size = 2500
	channel_size = 3
	#event_array = torch.zeros(station_no,sample_size)
	event_array = torch.zeros(channel_size,station_no,sample_size)
	#sorted_event_array = torch.zeros(station_no,sample_size)
	#sorted_event_array = torch.zeros(channel_size,station_no,sample_size)
	max_amp_idx = []
	max_amp = []
	snr = []
	#i=0
	ii=0
	for r, d, f in os.walk(path):
		if f == []:
			return []
		for filename in sorted(f):
			i = ii // 3	
			j = ii % 3
			tr = load_data(os.path.join(r,filename), False)
			if len(tr.data) < sample_size:
				print('ERROR '+filename+' '+str(len(tr.data)))
			# only get vertical component
			#elif tr.stats['channel'] == 'HHZ':
			else:
				# pdb.set_trace()
				# idx = sorted_stations.index(tr.stats.station)
				# print(idx)
				#j = i % 3
				event_array[j,i,:] = torch.from_numpy(tr.data[:sample_size])
				#traces start 10 s or 500 sample points before the origin time
				# peak_amp = max(abs(event_array[j,i,500:]))
				peak_amp = max(abs(event_array[j,i,:]))
				#noise_amp = max(abs(event_array[j,i,50:450]))
				#snr_tmp = peak_amp / noise_amp
				#if snr_tmp < 2: 
				#	event_array[j,i,:] = event_array[j,i,:] * 0
				#else:
				#normalized 
				#	event_array[j,i,:] = event_array[j,i,:] / peak_amp
				event_array[j,i,:] = event_array[j,i,:] / peak_amp
				
				#snr.append(snr_tmp)
				# FG stands for funcgen, a random time series generated by SAC
				if tr.stats['network'] == 'FG':
					event_array[j,i,:] = event_array[j,i,:] * 0

				#i+=1
			ii+=1
			if i == station_no:
				break
	# ensure all values between 0 and 1
	#event_array = event_array / max(max_amp)
	#event_array = abs(event_array) / max(snr)
	# sort traces in order of when their maximum amplitude arrives
	#idx = np.argsort(max_amp_idx)
	# any stations not included (out of 28) will be the last row of zeros
	#sorted_event_array[:i,:] = event_array[idx,:]
	#sorted_event_array = abs(event_array)
	#print(sorted_event_array.shape)
	# take absolute
	#event_array = abs(event_array)
	
	if (showPlots):
		#fig, axs = plt.subplots(station_no, sharey="col")
		# show the first 15 traces
		fig, axs = plt.subplots(station_no, sharey="col")
		fig.suptitle(path)	
		#ii = 0
		for i in range(station_no):
			#j = ii % 3
			#i = ii // 3
			axs[i].plot(event_array[2,i,:])
			axs[i].axis('off')
		#for i in range(station_no):
			#axs[i].plot(sorted_event_array[:,i,:])
		plt.show()
		# plt.savefig("test")
	return event_array	

def separate_stations(path):
    SAC_filenames = find_all_SAC(path)
    #sample_size = 8000
    sample_size = 2500
    trace_data = {}
    for filename in SAC_filenames:
        tr = load_data(filename, False)
        if len(tr.data) < sample_size:
            print('ERROR '+filename+' '+str(len(tr.data)))
        else:
            if tr.stats.station not in trace_data:
                trace_data[tr.stats.station]= np.expand_dims(tr.data[0:sample_size],axis=0)
            else:
                trace_data[tr.stats.station]=np.append(trace_data[tr.stats.station],\
                                                       np.expand_dims(tr.data[0:sample_size],axis=0),\
                                                       axis=0)
    return trace_data


def get_coords(dirname):
	#earthquake_df = pd.read_csv('./EQinfo/eqs_2017.csv')
	#earthquake_df = pd.read_csv('/Users/yangshen/Proj.ML/EQinfo/eqs_2017and2019.csv')
	# to account for "unknown" eq origin time, the sac files for each event are sliced at a random time before origin
	# thus adding the 4th variable to be solved, see ./csh/add_TBO2csv.csh
	# earthquake_df = pd.read_csv('/Users/yshen/Proj.ML/EQinfo/eqs_2017_withTBO_Mcut50s.csv')
	earthquake_df = pd.read_csv('/Users/yshen/Proj.ML/EQinfo/Lin_2020_reloc_withTBO.csv')

	# uniqueID = dirname[-14:]
	uniqueID = dirname[-17:]
	# earthquake_df = earthquake_df.set_index(earthquake_df['time'].str[:19])
	earthquake_df = earthquake_df.set_index(earthquake_df['evtCutID'].str[:17])
	# match = uniqueID[0:4]+"-"+uniqueID[4:6]+"-"+uniqueID[6:8]+"T"+uniqueID[8:10]+":"+uniqueID[10:12]+":"+uniqueID[12:]
	match = uniqueID
	# print (match)
	# matching_row = earthquake_df.loc[earthquake_df['uniqueID'] == match]
	# print([earthquake_df['latitude'][match], earthquake_df['longitude'][match], earthquake_df['depth'][match], earthquake_df['timeBeforeOri'][match], earthquake_df['evtCutID'][match]])
	return torch.tensor([earthquake_df['latitude'][match], earthquake_df['longitude'][match], earthquake_df['depth'][match], earthquake_df['timeBeforeOri'][match]])

if __name__ == "__main__":
	device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
	#pos_path = "/Volumes/jd/data.hawaii/data_prepared_3c.sac/"
	# screened earthquake data, by number of stations recorded and azimuthal gaps
	#pos_path = "/Volumes/jd/data.hawaii/data_prepared_FCreviewedRandScreened"
	# not screened
	# pos_path = "/Volumes/jd/data.hawaii/data_prepared_FCreviewedRand"
	# pos_path = "/Volumes/jd/data.hawaii/data_prepared_FCreviewedRandMcut50s"
	pos_path = "/Volumes/jd/data.hawaii/data_prepared_LinReloc"

	#sample_size = 8000
	# sample_size = 2000
	sample_size = 2500
	pos_dirs = find_all_events(pos_path)
	print(len(pos_dirs))
	station_no = 55
	channel_size = 3

	#X_all = torch.zeros(len(pos_dirs),station_no, sample_size)
	X_all = torch.zeros(len(pos_dirs),channel_size,station_no, sample_size)
	#y_all = torch.zeros(len(pos_dirs),3)
	# 4d: lat, long, dep, time
	y_all = torch.zeros(len(pos_dirs),4)

	for i,dirname in enumerate(pos_dirs):
		print(dirname)
		#event_coordref = (19.5,-155.5,0)
		#event_norm = (1.0,1.0,100.0)
		event_coordref = (19.5,-155.5,0.0,0.0)
		# event_norm = (1.0,1.0,50.0,15.0)
		event_norm = (1.0,1.0,50.0,10.0)

		event_array = get_event(dirname, station_no)
		event_coordinates = get_coords(dirname)
		#print(event_coordinates.shape)
		# normalize location and time
		event_coordinates = np.subtract(event_coordinates, event_coordref)
		event_coordinates = np.divide(event_coordinates, event_norm) 	
		#X_all[i,:,:] = event_array
		X_all[i,:,:,:] = event_array
		y_all[i,:] = event_coordinates
	X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.25, random_state=42)

	#torch.save((X_train, y_train), 'train_data.pt')
	#torch.save((X_test, y_test), 'test_data.pt')
	torch.save((X_train, y_train), '/Volumes/jd/data.hawaii/pts/train_data3c4d_NotAbs2017Mcut50sLin.pt')
	torch.save((X_test, y_test), '/Volumes/jd/data.hawaii/pts/test_data3c4d_NotAbs2017Mcut50sLin.pt')



